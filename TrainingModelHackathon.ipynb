{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary libraries\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import glob\n",
    "import json\n",
    "import matplotlib as rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load metadata.csv in the CORD-19 dataset and save it into a dataframe \n",
    "\n",
    "meta_data = pd.read_csv('filepath/metadata.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#info about the dataframe\n",
    "meta_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing the data\n",
    "\n",
    "#handling duplicate data (based on 'sha','title' and 'abstract')\n",
    "print(meta_data[meta_data.duplicated(subset=['sha','title','abstract'], keep=False) == True])\n",
    "meta_data.drop_duplicates(subset=['sha','title','abstract'],keep ='last',inplace=True)\n",
    "print('Data Size after dropping duplicated data (based on abstract attribute):',meta_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to deal with null values\n",
    "#'No Information Available' will be replaced \n",
    "def dealing_with_null_values(dataset):\n",
    "    dataset = dataset\n",
    "    for i in dataset.columns:\n",
    "        replace = []\n",
    "        data  = dataset[i].isnull()\n",
    "        count = 0\n",
    "        for j,k in zip(data,dataset[i]):\n",
    "            if (j==True):\n",
    "                count = count+1\n",
    "                replace.append('No Information Available')\n",
    "            else:\n",
    "                replace.append(k)\n",
    "        print(\"Num of null values (\",i,\"):\",count)\n",
    "        dataset[i] = replace\n",
    "    return dataset\n",
    "\n",
    "meta_data = dealing_with_null_values(meta_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Document Vectorization using doc2vec from gensim\n",
    "\n",
    "#importt gensim and doc2vec\n",
    "import gensim\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "def doc2vec():\n",
    "    document_tagged = []\n",
    "    tagged_count = 0\n",
    "    for _ in meta_data['abstract'].values:\n",
    "        document_tagged.append(gensim.models.doc2vec.TaggedDocument(_,[tagged_count]))\n",
    "        tagged_count +=1 \n",
    "    d2v = Doc2Vec(document_tagged)\n",
    "    d2v.train(document_tagged,epochs=d2v.epochs,total_examples=d2v.corpus_count)\n",
    "    return d2v.docvecs.vectors_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# data representation of doc2vec for abstract column.\n",
    "# Visualization the doc2vec representation\n",
    "%time doc2vec = doc2vec()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#heatmap for visualization\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(16,16))\n",
    "sns.heatmap(doc2vec,cmap=\"coolwarm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for clustering the documents we use Kmeans as our clustering algorithm\n",
    "\n",
    "# importing KMeans library of sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def kmeans(n_clusters):\n",
    "    kmean_model = KMeans(n_clusters = n_clusters,random_state=0)\n",
    "    return kmean_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = doc2vec\n",
    "kmeans5 = KMeans(5)\n",
    "\n",
    "%time km5 = kmeans5.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans6 = KMeans(6)\n",
    "%time km6 = kmeans6.fit_predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans7 = KMeans(7)\n",
    "%time km7 = kmeans7.fit_predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans10 = KMeans(10)\n",
    "%time km10 = kmeans10.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans4= KMeans(4)\n",
    "%time km4 = kmeans4.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans3= KMeans(3)\n",
    "%time km3 = kmeans3.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans2= KMeans(2)\n",
    "%time km2 = kmeans2.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans1= KMeans(1)\n",
    "%time km1 = kmeans1.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Further analysis of clusters\n",
    "models = [kmeans1, kmeans2, kmeans3,kmeans4, kmeans5, kmeans6, kmeans7, kmeans10]\n",
    "def plot_WCSS_BCSS(models, data):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12,5))\n",
    "    \n",
    "    ## Plot WCSS\n",
    "    wcss = [mod.inertia_ for mod in models]\n",
    "    n_clusts = [1,2, 3,4, 5,6,7, 10]\n",
    "    \n",
    "    ax[0].bar(n_clusts, wcss,color='orange', edgecolor='black', linewidth=1)\n",
    "    ax[0].set_xlabel('Number of clusters')\n",
    "    ax[0].set_ylabel('WCSS')\n",
    "    ax[0].set_title('Within Cluster Analysis')\n",
    "    \n",
    "    \n",
    "    ## Plot BCSS \n",
    "    n_1 = (float(data.shape[0]) * float(data.shape[1])) - 1.0\n",
    "    tss = n_1 * np.var(data)\n",
    "    bcss = [tss - x for x in wcss]\n",
    "    ax[1].bar(n_clusts, bcss,edgecolor='black')\n",
    "    ax[1].set_xlabel('Number of clusters')\n",
    "    ax[1].set_ylabel('BCSS')\n",
    "    ax[1].set_title('Between Cluster Analysis')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "plot_WCSS_BCSS(models,X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating Silhouette coefficients for choosing the number of clusters for our model\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def plot_silhouette(kms,data,nclusts):\n",
    "    \n",
    "    silhouette = []\n",
    "    for i in kms:\n",
    "        score = silhouette_score(data,i)\n",
    "        print(score)\n",
    "        silhouette.append(score)\n",
    "    \n",
    "    \n",
    "    plt.bar(nclusts, silhouette,color = 'green')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n",
    "%time plot_silhouette([km1, km2, km3, km4,km5,km6,km7, km10],X,[1,2,3,4,5,6,7,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time plot_silhouette([km4,km7,km10],X,[4,7,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time plot_silhouette([km3,km4,km5],X,[3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time plot_silhouette([km2],X,[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time plot_silhouette([km1],X,[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Silhoute constant of 2 clusters came out closed to +1\n",
    "meta_data['cluster_doc2vec_kmeans2'] = kmeans2.labels_\n",
    "pca = PCA(n_components=2).fit(X)\n",
    "data = pca.transform(X)\n",
    "centroids =  pca.transform(kmeans2.cluster_centers_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2).fit(X)\n",
    "data = pca.transform(X)\n",
    "centroids =  pca.transform(kmeans2.cluster_centers_)\n",
    "plt.scatter(data[:, 0], data[:, 1],c = color)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], marker='*', s=200, c='#000000')\n",
    "plt.title(\"Doc2Vec Matrix with 2 clusters_2Dimension\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe =pd.DataFrame()\n",
    "dataframe['cluster'] = meta_data['cluster_doc2vec_kmeans2']\n",
    "dataframe['x'] =data[:, 0]\n",
    "dataframe['y'] =data[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final dataframe after clustering\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are using ludwig text classificattion to train our classify our model according to the two clusters.\n",
    "#We used parallel-cnn as our encoder\n",
    "#input were the abstracts of all the papers from the dataset and classifies based on cluster number determined. \n",
    "!ludwig experiment \\\n",
    "  --data_csv datframe_with_two_clusters.csv \\\n",
    "  --model_definition_file model_definition_cord.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy of training set = 88.1%\n",
    "#accuracy of validation set = 74.1%\n",
    "#accuracy of test set = 74.4%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizations\n",
    "!ludwig visualize -v learning_curves --training_statistics results/experiment_run_3/training_statistics.json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/Training_clusters_hitsatk.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/Training_clusters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/training_clsuters_combines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/Training_Clusters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/training_clusters_combined.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
